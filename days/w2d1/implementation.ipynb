{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.nn.functional import gelu, softmax, dropout\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Raw Attention Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first make the part of the attention layer that computes the raw attention scores (pre-softmax) between each pair of tokens. Write a function to do this, called `raw_attention_scores`. \n",
    "Type signature of `raw_attention_scores`:\n",
    "```\n",
    "token_activations: Tensor[batch_size, seq_length, hidden_size (which is 768)], \n",
    "num_heads: int, \n",
    "project_query: function(Tensor[..., 768] -> Tensor[..., num_heads*head_size]), \n",
    "project_key: function(Tensor[..., 768] -> Tensor[..., num_heads*head_size])\n",
    "-> \n",
    "Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "```\n",
    "If the dimensions of project_query and project_key functions don’t make sense, reread the general guidelines above. \n",
    "\n",
    "Gotcha: remember the “divide by sqrt(head_size)” from the Illustrated Transformer!\n",
    "Gotcha #2: \"raw attention pattern\" means pre-softmax (otherwise known as \"attention score\").\n",
    "\n",
    "Test your function with `bert_tests.test_attention_pattern_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.01208 STD: 0.1096 VALS [0.05786 0.0006444 0.0845 0.01998 -0.02516 -0.05008 -0.0319 -0.04448 0.09316 0.06063...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(\n",
    "        token_activations,  # Tensor[batch_size, seq_length, hidden_size(768)],\n",
    "        num_heads,\n",
    "        project_query,      # nn.Module, (Tensor[..., 768]) -> Tensor[..., 768],\n",
    "        project_key,        # nn.Module, (Tensor[..., 768]) -> Tensor[..., 768]\n",
    "): # -> Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]:\n",
    "    head_size = token_activations.shape[-1] / num_heads\n",
    "\n",
    "    Q = project_query(token_activations)\n",
    "    Q = rearrange(Q, 'b seq_length (num_head head_sz) -> b num_head seq_length head_sz', num_head=num_heads)\n",
    "    K = project_key(token_activations)\n",
    "    K = rearrange(K, 'b seq_length (num_head head_sz) -> b num_head seq_length head_sz', num_head=num_heads)\n",
    "\n",
    "    A = einsum('bhql,bhkl->bhkq', Q, K) / np.sqrt(head_size)\n",
    "\n",
    "    return A\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3720, 0.4221, 0.4499],\n",
      "        [0.4408, 0.3297, 0.3393],\n",
      "        [0.1872, 0.2482, 0.2107]])\n",
      "torch.Size([2, 12, 3, 3]) torch.Size([2, 12, 3, 64])\n",
      "torch.Size([2, 12, 3, 64])\n",
      "torch.Size([2, 3, 768])\n",
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.004708 STD: 0.1157 VALS [-0.1737 -0.04187 -0.03834 0.02038 0.0409 -0.07649 -0.1073 0.04715 -0.04157 -0.01852...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "        token_activations, #: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "        num_heads: int,\n",
    "        attention_pattern, #: Tensor[batch_size,num_heads, seq_length, seq_length],\n",
    "        project_value, # nn.Module, (Tensor[..., 768]) -> Tensor[..., 768],\n",
    "        project_output, # nn.Module, (Tensor[..., 768]) -> Tensor[..., 768],\n",
    "): #-> Tensor[batch_size, seq_length, hidden_size]))\n",
    "    V = project_value(token_activations)\n",
    "    V = rearrange(V, 'b seq_length (num_head head_sz) -> b num_head seq_length head_sz', num_head=num_heads)\n",
    "    A = softmax(attention_pattern, dim=2)\n",
    "    out = einsum('bhkq,bhkn->bhqn', A, V)\n",
    "    out = rearrange(out, 'b h q n -> b q (h n)')\n",
    "    return project_output(out)\n",
    "bert_tests.test_attention_fn(bert_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3454, 0.3640, 0.3404],\n",
      "        [0.3531, 0.3300, 0.3391],\n",
      "        [0.3015, 0.3060, 0.3205]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([2, 12, 3, 3]) torch.Size([2, 12, 3, 64])\n",
      "torch.Size([2, 12, 3, 64])\n",
      "torch.Size([2, 3, 768])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        head_size = 64\n",
    "        attention_hidden_size = num_heads * head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.project_query = t.nn.Linear(hidden_size, attention_hidden_size)\n",
    "        self.project_key = t.nn.Linear(hidden_size, attention_hidden_size)\n",
    "        self.project_value = t.nn.Linear(hidden_size, attention_hidden_size)\n",
    "        self.project_output = t.nn.Linear(attention_hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_pattern = raw_attention_pattern(x, self.num_heads, self.project_query, self.project_key)\n",
    "        head_out = bert_attention(x, self.num_heads, attention_pattern, self.project_value, self.project_output)\n",
    "        return head_out\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(\n",
    "        token_activations, #: torch.Tensor[batch_size,seq_length,768],\n",
    "        linear_1, #: nn.Module,\n",
    "        linear_2, #: nn.Module\n",
    "    ): # -> torch.Tensor[batch_size, seq_length, 768]\n",
    "    return linear_2(gelu(linear_1(token_activations)))\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = t.nn.Linear(input_size, intermediate_size)\n",
    "        self.linear2 = t.nn.Linear(intermediate_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return bert_mlp(x, self.linear1, self.linear2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: 9.537e-09 STD: 1.003 VALS [-1.352 1.454 -0.5328 1.027 1.477 -0.1402 -1.172 -0.5576 -0.7403 0.5375...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, size_of_normalized_dim):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones(size_of_normalized_dim))\n",
    "        self.bias = t.nn.Parameter(t.zeros(size_of_normalized_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5\n",
    "        x = (x - x.mean(dim=-1, keepdim=True)) / (x.var(dim=-1, keepdim=True, unbiased=False) + eps).sqrt()\n",
    "        x = x * self.weight + self.bias\n",
    "        return x\n",
    "\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3195, 0.3199, 0.3158],\n",
      "        [0.3304, 0.3499, 0.3461],\n",
      "        [0.3501, 0.3302, 0.3381]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([2, 12, 3, 3]) torch.Size([2, 12, 3, 64])\n",
      "torch.Size([2, 12, 3, 64])\n",
      "torch.Size([2, 3, 768])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 2.07e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.layernorm1 = LayerNorm(hidden_size)\n",
    "        self.layernorm2 = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.layernorm1(self.attention(x) + x)\n",
    "        out2 = self.layernorm2(self.dropout(self.mlp(out1) + out1))\n",
    "        return out2\n",
    "\n",
    "bert_tests.test_bert_block(BertBlock)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.2095 STD: 0.8819 VALS [-0.8435 0.0199 -0.7648 1.023 -1.396 -0.8435 0.0199 -0.7648 1.023 -1.396...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn(vocab_size, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding[x]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def bert_embedding(\n",
    "        input_ids, # [batch, seqlen]\n",
    "        token_type_ids, # [batch, seqlen]\n",
    "        position_embedding: Embedding,\n",
    "        token_embedding: Embedding,\n",
    "        token_type_embedding: Embedding,\n",
    "        layer_norm: LayerNorm,\n",
    "        dropout: t.nn.Dropout):\n",
    "    pass\n",
    "    # batch, seqlen = input_ids.shape\n",
    "    # position_idxs = t.arange(end=batch)\n",
    "    # token_embeds = token_embedding[input_ids]\n",
    "    # token_type_embeds = token_type_embedding[token_type_ids]\n",
    "    #\n",
    "    # position_embeds = position_embedding[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d73fc3eac8eb1b6160a9b29e526addea4bb76d1fc003d28413226486a77224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}